{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6706d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port','0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/itv007008/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9911c449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://g02.itversity.com:40501\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff4bbd31fd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "253edbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"header\",\"true\") \\\n",
    ".option(\"inferSchema\", \"true\") \\\n",
    ".load(\"/public/trendytech/orders_wh/orders_wh.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c856363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|   order_status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|       9198|     PROCESSING|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97388420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e0cd70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df1 = orders_df.withColumnRenamed(\"order_status\", \"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fed449a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|         status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|       9198|     PROCESSING|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0409968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "transformed_df2 = transformed_df1.withColumn(\"order_date_new\", to_timestamp(\"order_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53505294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+-------------------+\n",
      "|order_id|          order_date|customer_id|         status|     order_date_new|\n",
      "+--------+--------------------+-----------+---------------+-------------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|2013-07-25 00:00:00|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|2013-07-25 00:00:00|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|2013-07-25 00:00:00|\n",
      "|       4|2013-07-25 00:00:...|       8827|         CLOSED|2013-07-25 00:00:00|\n",
      "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|2013-07-25 00:00:00|\n",
      "|       6|2013-07-25 00:00:...|       7130|       COMPLETE|2013-07-25 00:00:00|\n",
      "|       7|2013-07-25 00:00:...|       4530|       COMPLETE|2013-07-25 00:00:00|\n",
      "|       8|2013-07-25 00:00:...|       2911|     PROCESSING|2013-07-25 00:00:00|\n",
      "|       9|2013-07-25 00:00:...|       5657|PENDING_PAYMENT|2013-07-25 00:00:00|\n",
      "|      10|2013-07-25 00:00:...|       5648|PENDING_PAYMENT|2013-07-25 00:00:00|\n",
      "|      11|2013-07-25 00:00:...|        918| PAYMENT_REVIEW|2013-07-25 00:00:00|\n",
      "|      12|2013-07-25 00:00:...|       1837|         CLOSED|2013-07-25 00:00:00|\n",
      "|      13|2013-07-25 00:00:...|       9149|PENDING_PAYMENT|2013-07-25 00:00:00|\n",
      "|      14|2013-07-25 00:00:...|       9842|     PROCESSING|2013-07-25 00:00:00|\n",
      "|      15|2013-07-25 00:00:...|       2568|       COMPLETE|2013-07-25 00:00:00|\n",
      "|      16|2013-07-25 00:00:...|       7276|PENDING_PAYMENT|2013-07-25 00:00:00|\n",
      "|      17|2013-07-25 00:00:...|       2667|       COMPLETE|2013-07-25 00:00:00|\n",
      "|      18|2013-07-25 00:00:...|       1205|         CLOSED|2013-07-25 00:00:00|\n",
      "|      19|2013-07-25 00:00:...|       9488|PENDING_PAYMENT|2013-07-25 00:00:00|\n",
      "|      20|2013-07-25 00:00:...|       9198|     PROCESSING|2013-07-25 00:00:00|\n",
      "+--------+--------------------+-----------+---------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2699eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- order_date_new: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a14b5feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_reader_df = spark.read.csv(\"/public/trendytech/orders_wh/orders_wh.csv\", header = \"true\", inferSchema = \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b567c4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|   order_status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|       9198|     PROCESSING|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_reader_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5747c546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_reader_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1e80c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_reader_df=spark.read.json(\"/public/trendytech/datasets/orders.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d7c8e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------+---------------+\n",
      "|customer_id|          order_date|order_id|   order_status|\n",
      "+-----------+--------------------+--------+---------------+\n",
      "|      11599|2013-07-25 00:00:...|       1|         CLOSED|\n",
      "|        256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n",
      "|      12111|2013-07-25 00:00:...|       3|       COMPLETE|\n",
      "|       8827|2013-07-25 00:00:...|       4|         CLOSED|\n",
      "|      11318|2013-07-25 00:00:...|       5|       COMPLETE|\n",
      "|       7130|2013-07-25 00:00:...|       6|       COMPLETE|\n",
      "|       4530|2013-07-25 00:00:...|       7|       COMPLETE|\n",
      "|       2911|2013-07-25 00:00:...|       8|     PROCESSING|\n",
      "|       5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|\n",
      "|       5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|\n",
      "|        918|2013-07-25 00:00:...|      11| PAYMENT_REVIEW|\n",
      "|       1837|2013-07-25 00:00:...|      12|         CLOSED|\n",
      "|       9149|2013-07-25 00:00:...|      13|PENDING_PAYMENT|\n",
      "|       9842|2013-07-25 00:00:...|      14|     PROCESSING|\n",
      "|       2568|2013-07-25 00:00:...|      15|       COMPLETE|\n",
      "|       7276|2013-07-25 00:00:...|      16|PENDING_PAYMENT|\n",
      "|       2667|2013-07-25 00:00:...|      17|       COMPLETE|\n",
      "|       1205|2013-07-25 00:00:...|      18|         CLOSED|\n",
      "|       9488|2013-07-25 00:00:...|      19|PENDING_PAYMENT|\n",
      "|       9198|2013-07-25 00:00:...|      20|     PROCESSING|\n",
      "+-----------+--------------------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_reader_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7502c500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_reader_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c80589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_readers_df = spark.read.parquet(\"/public/trendytech/datasets/ordersparquet/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53b7eeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------+---------------+\n",
      "|customer_id|          order_date|order_id|   order_status|\n",
      "+-----------+--------------------+--------+---------------+\n",
      "|      11599|2013-07-25 00:00:...|       1|         CLOSED|\n",
      "|        256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n",
      "|      12111|2013-07-25 00:00:...|       3|       COMPLETE|\n",
      "|       8827|2013-07-25 00:00:...|       4|         CLOSED|\n",
      "|      11318|2013-07-25 00:00:...|       5|       COMPLETE|\n",
      "|       7130|2013-07-25 00:00:...|       6|       COMPLETE|\n",
      "|       4530|2013-07-25 00:00:...|       7|       COMPLETE|\n",
      "|       2911|2013-07-25 00:00:...|       8|     PROCESSING|\n",
      "|       5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|\n",
      "|       5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|\n",
      "|        918|2013-07-25 00:00:...|      11| PAYMENT_REVIEW|\n",
      "|       1837|2013-07-25 00:00:...|      12|         CLOSED|\n",
      "|       9149|2013-07-25 00:00:...|      13|PENDING_PAYMENT|\n",
      "|       9842|2013-07-25 00:00:...|      14|     PROCESSING|\n",
      "|       2568|2013-07-25 00:00:...|      15|       COMPLETE|\n",
      "|       7276|2013-07-25 00:00:...|      16|PENDING_PAYMENT|\n",
      "|       2667|2013-07-25 00:00:...|      17|       COMPLETE|\n",
      "|       1205|2013-07-25 00:00:...|      18|         CLOSED|\n",
      "|       9488|2013-07-25 00:00:...|      19|PENDING_PAYMENT|\n",
      "|       9198|2013-07-25 00:00:...|      20|     PROCESSING|\n",
      "+-----------+--------------------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquet_readers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dce61918",
   "metadata": {},
   "outputs": [],
   "source": [
    "orc_readers_df = spark.read.orc(\"/public/trendytech/datasets/ordersorc/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdc3fe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------+---------------+\n",
      "|customer_id|          order_date|order_id|   order_status|\n",
      "+-----------+--------------------+--------+---------------+\n",
      "|      11599|2013-07-25 00:00:...|       1|         CLOSED|\n",
      "|        256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n",
      "|      12111|2013-07-25 00:00:...|       3|       COMPLETE|\n",
      "|       8827|2013-07-25 00:00:...|       4|         CLOSED|\n",
      "|      11318|2013-07-25 00:00:...|       5|       COMPLETE|\n",
      "|       7130|2013-07-25 00:00:...|       6|       COMPLETE|\n",
      "|       4530|2013-07-25 00:00:...|       7|       COMPLETE|\n",
      "|       2911|2013-07-25 00:00:...|       8|     PROCESSING|\n",
      "|       5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|\n",
      "|       5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|\n",
      "|        918|2013-07-25 00:00:...|      11| PAYMENT_REVIEW|\n",
      "|       1837|2013-07-25 00:00:...|      12|         CLOSED|\n",
      "|       9149|2013-07-25 00:00:...|      13|PENDING_PAYMENT|\n",
      "|       9842|2013-07-25 00:00:...|      14|     PROCESSING|\n",
      "|       2568|2013-07-25 00:00:...|      15|       COMPLETE|\n",
      "|       7276|2013-07-25 00:00:...|      16|PENDING_PAYMENT|\n",
      "|       2667|2013-07-25 00:00:...|      17|       COMPLETE|\n",
      "|       1205|2013-07-25 00:00:...|      18|         CLOSED|\n",
      "|       9488|2013-07-25 00:00:...|      19|PENDING_PAYMENT|\n",
      "|       9198|2013-07-25 00:00:...|      20|     PROCESSING|\n",
      "+-----------+--------------------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orc_readers_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c10368a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = orc_readers_df.where(\"customer_id=11599\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "010359dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------+--------+------------+\n",
      "|customer_id|order_date           |order_id|order_status|\n",
      "+-----------+---------------------+--------+------------+\n",
      "|11599      |2013-07-25 00:00:00.0|1       |CLOSED      |\n",
      "|11599      |2013-10-03 00:00:00.0|11397   |COMPLETE    |\n",
      "|11599      |2013-12-20 00:00:00.0|23908   |COMPLETE    |\n",
      "|11599      |2014-06-27 00:00:00.0|53545   |PENDING     |\n",
      "|11599      |2013-10-17 00:00:00.0|59911   |PROCESSING  |\n",
      "+-----------+---------------------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "971d2e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = orc_readers_df.filter(\"customer_id=11599\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90e3e09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------+------------+\n",
      "|customer_id|          order_date|order_id|order_status|\n",
      "+-----------+--------------------+--------+------------+\n",
      "|      11599|2013-07-25 00:00:...|       1|      CLOSED|\n",
      "|      11599|2013-10-03 00:00:...|   11397|    COMPLETE|\n",
      "|      11599|2013-12-20 00:00:...|   23908|    COMPLETE|\n",
      "|      11599|2014-06-27 00:00:...|   53545|     PENDING|\n",
      "|      11599|2013-10-17 00:00:...|   59911|  PROCESSING|\n",
      "+-----------+--------------------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e96d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab3a3ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = spark.sql(\"Select * from orders where order_status = 'CLOSED'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d5f02b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+------------+\n",
      "|order_id|          order_date|customer_id|order_status|\n",
      "+--------+--------------------+-----------+------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|      CLOSED|\n",
      "|       4|2013-07-25 00:00:...|       8827|      CLOSED|\n",
      "|      12|2013-07-25 00:00:...|       1837|      CLOSED|\n",
      "|      18|2013-07-25 00:00:...|       1205|      CLOSED|\n",
      "|      24|2013-07-25 00:00:...|      11441|      CLOSED|\n",
      "|      25|2013-07-25 00:00:...|       9503|      CLOSED|\n",
      "|      37|2013-07-25 00:00:...|       5863|      CLOSED|\n",
      "|      51|2013-07-25 00:00:...|      12271|      CLOSED|\n",
      "|      57|2013-07-25 00:00:...|       7073|      CLOSED|\n",
      "|      61|2013-07-25 00:00:...|       4791|      CLOSED|\n",
      "|      62|2013-07-25 00:00:...|       9111|      CLOSED|\n",
      "|      87|2013-07-25 00:00:...|       3065|      CLOSED|\n",
      "|      90|2013-07-25 00:00:...|       9131|      CLOSED|\n",
      "|     101|2013-07-25 00:00:...|       5116|      CLOSED|\n",
      "|     116|2013-07-26 00:00:...|       8763|      CLOSED|\n",
      "|     129|2013-07-26 00:00:...|       9937|      CLOSED|\n",
      "|     133|2013-07-26 00:00:...|      10604|      CLOSED|\n",
      "|     191|2013-07-26 00:00:...|         16|      CLOSED|\n",
      "|     201|2013-07-26 00:00:...|       9055|      CLOSED|\n",
      "|     211|2013-07-26 00:00:...|      10372|      CLOSED|\n",
      "+--------+--------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27489189",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersdf = spark.read.table(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21e72266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|   order_status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|       9198|     PROCESSING|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc61162e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## How to create a spark table ###\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS itv007008_retail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a6de64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|   0001_av_ivy_tesco|\n",
      "|        003402_hive1|\n",
      "|    005198_ivy_tesco|\n",
      "|    005212_ivy_tesco|\n",
      "| 005222_ivy_practice|\n",
      "| 005260_ivy_database|\n",
      "|    005302_retail_db|\n",
      "|005876_week5_assi...|\n",
      "|       005933_retail|\n",
      "|     006586_database|\n",
      "|     006608_database|\n",
      "|006866_week5_assi...|\n",
      "|         00ivy_tesco|\n",
      "|          00ivy_test|\n",
      "|       07172021_nyse|\n",
      "|     07172021_retail|\n",
      "|        07172021_sms|\n",
      "|         1230_trendy|\n",
      "|     1230_trendytech|\n",
      "|       1540retail_db|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23ae3b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|       namespace|\n",
      "+----------------+\n",
      "|itv007008_retail|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").filter(\"namespace like 'itv007008%'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "430c82e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-----------+\n",
      "|database|        tableName|isTemporary|\n",
      "+--------+-----------------+-----------+\n",
      "| default|            1htab|      false|\n",
      "| default|   41group_movies|      false|\n",
      "| default|    4group_movies|      false|\n",
      "| default|             4tab|      false|\n",
      "| default|    6_flags_simon|      false|\n",
      "| default|               aa|      false|\n",
      "| default|              abc|      false|\n",
      "| default|             acid|      false|\n",
      "| default|            acid1|      false|\n",
      "| default|     acid_example|      false|\n",
      "| default|    acid_example1|      false|\n",
      "| default|    acid_example2|      false|\n",
      "| default|           adata1|      false|\n",
      "| default|        adata_ell|      false|\n",
      "| default|         adata_vr|      false|\n",
      "| default|    ad_earthquake|      false|\n",
      "| default|ad_earthquake_par|      false|\n",
      "| default|           adelta|      false|\n",
      "| default|       adeltapart|      false|\n",
      "| default|   adeltapartbuck|      false|\n",
      "+--------+-----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e9276dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use itv007008_retail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "242b0928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+-----------+\n",
      "|        database| tableName|isTemporary|\n",
      "+----------------+----------+-----------+\n",
      "|itv007008_retail|    orders|      false|\n",
      "|itv007008_retail|orders_ext|      false|\n",
      "|                |    orders|       true|\n",
      "+----------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "322b32fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE itv007008_retail.ORDERS(order_id integer, order_date string, customer_id integer, order_status string)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "056eb87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+-----------+\n",
      "|        database|tableName|isTemporary|\n",
      "+----------------+---------+-----------+\n",
      "|itv007008_retail|   orders|      false|\n",
      "|                |   orders|       true|\n",
      "+----------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c102cb72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"insert into itv007008_retail.ORDERS select * from orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "015ea009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+------------+\n",
      "|order_id|          order_date|customer_id|order_status|\n",
      "+--------+--------------------+-----------+------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|      CLOSED|\n",
      "|       4|2013-07-25 00:00:...|       8827|      CLOSED|\n",
      "|      12|2013-07-25 00:00:...|       1837|      CLOSED|\n",
      "|      18|2013-07-25 00:00:...|       1205|      CLOSED|\n",
      "|      24|2013-07-25 00:00:...|      11441|      CLOSED|\n",
      "|      25|2013-07-25 00:00:...|       9503|      CLOSED|\n",
      "|      37|2013-07-25 00:00:...|       5863|      CLOSED|\n",
      "|      51|2013-07-25 00:00:...|      12271|      CLOSED|\n",
      "|      57|2013-07-25 00:00:...|       7073|      CLOSED|\n",
      "|      61|2013-07-25 00:00:...|       4791|      CLOSED|\n",
      "|      62|2013-07-25 00:00:...|       9111|      CLOSED|\n",
      "|      87|2013-07-25 00:00:...|       3065|      CLOSED|\n",
      "|      90|2013-07-25 00:00:...|       9131|      CLOSED|\n",
      "|     101|2013-07-25 00:00:...|       5116|      CLOSED|\n",
      "|     116|2013-07-26 00:00:...|       8763|      CLOSED|\n",
      "|     129|2013-07-26 00:00:...|       9937|      CLOSED|\n",
      "|     133|2013-07-26 00:00:...|      10604|      CLOSED|\n",
      "|     191|2013-07-26 00:00:...|         16|      CLOSED|\n",
      "|     201|2013-07-26 00:00:...|       9055|      CLOSED|\n",
      "|     211|2013-07-26 00:00:...|      10372|      CLOSED|\n",
      "+--------+--------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select * from itv007008_retail.ORDERS where order_status='CLOSED'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31fb9fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use itv007008_retail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b79c0c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+-----------+\n",
      "|        database|tableName|isTemporary|\n",
      "+----------------+---------+-----------+\n",
      "|itv007008_retail|   orders|      false|\n",
      "|                |   orders|       true|\n",
      "+----------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f40dfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------+\n",
      "|    col_name|data_type|comment|\n",
      "+------------+---------+-------+\n",
      "|    order_id|      int|   null|\n",
      "|  order_date|   string|   null|\n",
      "| customer_id|      int|   null|\n",
      "|order_status|   string|   null|\n",
      "+------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe table itv007008_retail.orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29382b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                        |comment|\n",
      "+----------------------------+---------------------------------------------------------------------------------+-------+\n",
      "|order_id                    |int                                                                              |null   |\n",
      "|order_date                  |string                                                                           |null   |\n",
      "|customer_id                 |int                                                                              |null   |\n",
      "|order_status                |string                                                                           |null   |\n",
      "|                            |                                                                                 |       |\n",
      "|# Detailed Table Information|                                                                                 |       |\n",
      "|Database                    |itv007008_retail                                                                 |       |\n",
      "|Table                       |orders                                                                           |       |\n",
      "|Owner                       |itv007008                                                                        |       |\n",
      "|Created Time                |Fri Jun 09 22:22:32 EDT 2023                                                     |       |\n",
      "|Last Access                 |UNKNOWN                                                                          |       |\n",
      "|Created By                  |Spark 3.0.1                                                                      |       |\n",
      "|Type                        |MANAGED                                                                          |       |\n",
      "|Provider                    |hive                                                                             |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1686363759]                                               |       |\n",
      "|Statistics                  |2999944 bytes                                                                    |       |\n",
      "|Location                    |hdfs://m01.itversity.com:9000/user/itv007008/warehouse/itv007008_retail.db/orders|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe                               |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                                         |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat                       |       |\n",
      "+----------------------------+---------------------------------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe extended itv007008_retail.orders\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5ae1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table itv007008_retail.orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79d08f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th></th></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create table itv007008_retail.orders_ext(order_id integer, order_date string, customer_id integer, order_status string) using csv location '/public/trendytech/retail_db/orders'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21c84000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|   order_status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|       9198|     PROCESSING|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from itv007008_retail.orders_ext\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d623c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                       |comment|\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "|order_id                    |int                                                             |null   |\n",
      "|order_date                  |string                                                          |null   |\n",
      "|customer_id                 |int                                                             |null   |\n",
      "|order_status                |string                                                          |null   |\n",
      "|                            |                                                                |       |\n",
      "|# Detailed Table Information|                                                                |       |\n",
      "|Database                    |itv007008_retail                                                |       |\n",
      "|Table                       |orders_ext                                                      |       |\n",
      "|Owner                       |itv007008                                                       |       |\n",
      "|Created Time                |Fri Jun 09 22:33:49 EDT 2023                                    |       |\n",
      "|Last Access                 |UNKNOWN                                                         |       |\n",
      "|Created By                  |Spark 3.0.1                                                     |       |\n",
      "|Type                        |EXTERNAL                                                        |       |\n",
      "|Provider                    |csv                                                             |       |\n",
      "|Location                    |hdfs://m01.itversity.com:9000/public/trendytech/retail_db/orders|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe              |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.SequenceFileInputFormat                |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat       |       |\n",
      "+----------------------------+----------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe extended itv007008_retail.orders_ext\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23c46686",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Operation not allowed: TRUNCATE TABLE on external tables: `itv007008_retail`.`orders_ext`;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-6ec5a5c3c5a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"truncate table itv007008_retail.orders_ext\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \"\"\"\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Operation not allowed: TRUNCATE TABLE on external tables: `itv007008_retail`.`orders_ext`;"
     ]
    }
   ],
   "source": [
    "spark.sql(\"truncate table itv007008_retail.orders_ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e9d7842",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o45.sql.\n: org.apache.hadoop.security.AccessControlException: Permission denied: user=itv007008, access=WRITE, inode=\"/public/trendytech/retail_db/orders\":itv005857:supergroup:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:496)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:336)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:360)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:239)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1909)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1893)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1852)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3407)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1161)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:739)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:532)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1020)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:948)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2952)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\n\tat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2426)\n\tat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2400)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1324)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1321)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1338)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1313)\n\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:354)\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:163)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:168)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:607)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:602)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=itv007008, access=WRITE, inode=\"/public/trendytech/retail_db/orders\":itv005857:supergroup:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:496)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:336)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:360)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:239)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1909)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1893)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1852)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3407)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1161)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:739)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:532)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1020)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:948)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2952)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1457)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1367)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy18.mkdirs(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:656)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy19.mkdirs(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2424)\n\t... 40 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-1779fdc047c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"insert into table itv007008_retail.orders_ext values(1111, '12-02-2023', 2222, 'CLOSED')\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \"\"\"\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o45.sql.\n: org.apache.hadoop.security.AccessControlException: Permission denied: user=itv007008, access=WRITE, inode=\"/public/trendytech/retail_db/orders\":itv005857:supergroup:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:496)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:336)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:360)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:239)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1909)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1893)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1852)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3407)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1161)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:739)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:532)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1020)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:948)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2952)\n\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)\n\tat org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)\n\tat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2426)\n\tat org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2400)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1324)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1321)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1338)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1313)\n\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2275)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:354)\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:163)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:168)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:607)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:602)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=itv007008, access=WRITE, inode=\"/public/trendytech/retail_db/orders\":itv005857:supergroup:drwxr-xr-x\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:496)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:336)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:360)\n\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:239)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1909)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1893)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1852)\n\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:60)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3407)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1161)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:739)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:532)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1020)\n\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:948)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1845)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2952)\n\n\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1457)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1367)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)\n\tat com.sun.proxy.$Proxy18.mkdirs(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:656)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)\n\tat com.sun.proxy.$Proxy19.mkdirs(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2424)\n\t... 40 more\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"insert into table itv007008_retail.orders_ext values(1111, '12-02-2023', 2222, 'CLOSED')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14c4e053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|   order_status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|       9198|     PROCESSING|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99607779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6247437",
   "metadata": {},
   "source": [
    "## Higher Level API's Demo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9505e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc408535",
   "metadata": {},
   "source": [
    "#### 1. Top 15 customers who placed most number of orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0cc6f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|customer_id|count|\n",
      "+-----------+-----+\n",
      "|       5897|   16|\n",
      "|        569|   16|\n",
      "|      12431|   16|\n",
      "|       6316|   16|\n",
      "|        221|   15|\n",
      "|       5654|   15|\n",
      "|       4320|   15|\n",
      "|       5283|   15|\n",
      "|      12284|   15|\n",
      "|       5624|   15|\n",
      "|      11689|   14|\n",
      "|       4249|   14|\n",
      "|       6248|   14|\n",
      "|       4517|   14|\n",
      "|       3710|   14|\n",
      "+-----------+-----+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result1 = orders_df.groupby(\"customer_id\").count().sort(\"count\", ascending = False).show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1f7c5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|customer_id|count|\n",
      "+-----------+-----+\n",
      "|       5897|   16|\n",
      "|      12431|   16|\n",
      "|        569|   16|\n",
      "|       6316|   16|\n",
      "|      12284|   15|\n",
      "|       5624|   15|\n",
      "|       5654|   15|\n",
      "|       4320|   15|\n",
      "|       5283|   15|\n",
      "|        221|   15|\n",
      "|       4517|   14|\n",
      "|       3708|   14|\n",
      "|       4116|   14|\n",
      "|      11689|   14|\n",
      "|       6248|   14|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select customer_id, count(order_id) as count from orders group by customer_id order by count DESC limit 15\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fc3e7a",
   "metadata": {},
   "source": [
    "#### 2. Find the number of orders under each order status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bf29c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|   order_status|count|\n",
      "+---------------+-----+\n",
      "|PENDING_PAYMENT|15030|\n",
      "|       COMPLETE|22899|\n",
      "|        ON_HOLD| 3798|\n",
      "| PAYMENT_REVIEW|  729|\n",
      "|     PROCESSING| 8275|\n",
      "|         CLOSED| 7556|\n",
      "|SUSPECTED_FRAUD| 1558|\n",
      "|        PENDING| 7610|\n",
      "|       CANCELED| 1428|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.groupby(\"order_status\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c45c80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|   order_status|count|\n",
      "+---------------+-----+\n",
      "|PENDING_PAYMENT|15030|\n",
      "|       COMPLETE|22899|\n",
      "|        ON_HOLD| 3798|\n",
      "| PAYMENT_REVIEW|  729|\n",
      "|     PROCESSING| 8275|\n",
      "|         CLOSED| 7556|\n",
      "|SUSPECTED_FRAUD| 1558|\n",
      "|        PENDING| 7610|\n",
      "|       CANCELED| 1428|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select order_status, count(order_id) as count from orders group by order_status\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63777237",
   "metadata": {},
   "source": [
    "#### 3. Number of active customers ( who placed atleast one order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e0f9723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|12405|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"Select count(DISTINCT customer_id) as count  from orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2169fe98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12405"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_df.select(\"customer_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746ef011",
   "metadata": {},
   "source": [
    "#### 4. Customers with most number of closed orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "419aea8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|customer_id|count|\n",
      "+-----------+-----+\n",
      "|       1833|    6|\n",
      "|       1687|    5|\n",
      "|       1363|    5|\n",
      "|       5493|    5|\n",
      "|       2403|    4|\n",
      "|       2768|    4|\n",
      "|      10263|    4|\n",
      "|       2236|    4|\n",
      "|      10111|    4|\n",
      "|        437|    4|\n",
      "|       4573|    4|\n",
      "|       3631|    4|\n",
      "|      12431|    4|\n",
      "|       4588|    4|\n",
      "|       1521|    4|\n",
      "|       7948|    4|\n",
      "|      10018|    4|\n",
      "|       5319|    4|\n",
      "|       2774|    4|\n",
      "|       7879|    4|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select customer_id, count(order_id) as count from orders where order_status = 'CLOSED' group by customer_id\").sort(\"count\", ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "404578ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|customer_id|count|\n",
      "+-----------+-----+\n",
      "|       1833|    6|\n",
      "|       1363|    5|\n",
      "|       1687|    5|\n",
      "|       5493|    5|\n",
      "|       7948|    4|\n",
      "|       2768|    4|\n",
      "|      10263|    4|\n",
      "|       2236|    4|\n",
      "|       2403|    4|\n",
      "|       7879|    4|\n",
      "|       4573|    4|\n",
      "|       7850|    4|\n",
      "|      12431|    4|\n",
      "|       1521|    4|\n",
      "|      10111|    4|\n",
      "|        437|    4|\n",
      "|      10018|    4|\n",
      "|       5319|    4|\n",
      "|       2774|    4|\n",
      "|       3631|    4|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.filter(\"order_status='CLOSED'\").groupby(\"customer_id\").count().sort(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a2e6a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84c2c07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
